{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc65ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: requests in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user201\\.virtualenvs\\artificial_streaming-yra5kdww\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install requests tqdm scikit-learn pandas numpy joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a472a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] ability_clusters.joblib\n",
      "Silhouette: 0.1735\n",
      "Cluster sizes & top abilities (head):\n",
      "  C00 size=88, top=[('swarm', 0.203), ('compound-eyes', 0.083), ('shield-dust', 0.068)]\n",
      "  C01 size=78, top=[('chlorophyll', 0.236), ('overgrow', 0.164), ('leaf-guard', 0.091)]\n",
      "  C02 size=205, top=[('run-away', 0.053), ('intimidate', 0.05), ('pickup', 0.039)]\n",
      "  C03 size=63, top=[('keen-eye', 0.217), ('big-pecks', 0.085), ('tangled-feet', 0.047)]\n",
      "  C04 size=56, top=[('shed-skin', 0.061), ('sand-veil', 0.061), ('sap-sipper', 0.049)]\n",
      "\n",
      "[新キャラ 推奨特性（クラスタベース）]\n",
      " クラスターID: 5\n",
      " 推奨特性TOP5: [('せいしんりょく', 0.102), ('こんじょう', 0.078), ('もうか', 0.078), ('シンクロ', 0.039), ('てつのこぶし', 0.039)]\n",
      " 例ポケモン: ['オーベム', 'キリキザン', 'ワンリキー', 'エレキブル', 'オトスパス', 'エースバーン']\n",
      " 近傍（名前 / 特性 / 距離）:\n",
      "  - ソウブレイズ | もらいび | d= 3.281\n",
      "  - コノヨザル | やるき, せいしんりょく | d= 3.351\n",
      "  - アチゲータ | もうか | d= 3.56\n",
      "  - マフィティフ | いかく, ばんけん | d= 3.823\n",
      "  - オコリザル | やるき, いかりのつぼ | d= 3.929\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# poke_ability_clustering.py\n",
    "#  - PokeAPIから取得 → 特徴量化 → PCA+KMeansクラスタリング（K自動探索）\n",
    "#  - クラスタごとの特性分布から新キャラの「推奨特性TOPk」を返す\n",
    "#  - 日本語出力対応（特性名 / ポケモン名）：out_lang=\"ja\" or \"ja-Hrkt\"\n",
    "# 依存:\n",
    "#   pip install requests tqdm scikit-learn pandas numpy joblib\n",
    "\n",
    "import os, time, sys, json, math, random\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import Counter\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import joblib\n",
    "\n",
    "BASE = \"https://pokeapi.co/api/v2\"\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"poke-ml-unsupervised/1.0\"})\n",
    "\n",
    "TYPE_ORDER = [\n",
    "    \"normal\",\"fire\",\"water\",\"electric\",\"grass\",\"ice\",\"fighting\",\"poison\",\"ground\",\n",
    "    \"flying\",\"psychic\",\"bug\",\"rock\",\"ghost\",\"dragon\",\"dark\",\"steel\",\"fairy\"\n",
    "]\n",
    "\n",
    "# ------------------------------\n",
    "# 0) Utilities (retry / sleep)\n",
    "# ------------------------------\n",
    "def _get(url, timeout=30, max_retry=3, sleep_between=0.2):\n",
    "    for i in range(max_retry):\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception:\n",
    "            if i == max_retry - 1:\n",
    "                raise\n",
    "            time.sleep(sleep_between * (i + 1))\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Fetch PokeAPI (with cache)\n",
    "# ------------------------------\n",
    "def fetch_all_pokemon_names(limit=2000) -> List[str]:\n",
    "    r = _get(f\"{BASE}/pokemon?limit={limit}&offset=0\")\n",
    "    return [x[\"name\"] for x in r.json()[\"results\"]]\n",
    "\n",
    "def fetch_pokemon_detail(name: str) -> Dict:\n",
    "    return _get(f\"{BASE}/pokemon/{name}\").json()\n",
    "\n",
    "def fetch_species_detail(name_or_id: str) -> Dict:\n",
    "    return _get(f\"{BASE}/pokemon-species/{name_or_id}\").json()\n",
    "\n",
    "def row_from_payload(poke: Dict, species: Dict) -> Dict:\n",
    "    # 主特性（is_hidden=false）複数ありうる\n",
    "    primary_abilities = [a[\"ability\"][\"name\"] for a in poke[\"abilities\"] if not a.get(\"is_hidden\", False)]\n",
    "\n",
    "    stats_map = {s[\"stat\"][\"name\"]: s[\"base_stat\"] for s in poke[\"stats\"]}\n",
    "    hp   = stats_map.get(\"hp\", np.nan)\n",
    "    atk  = stats_map.get(\"attack\", np.nan)\n",
    "    defe = stats_map.get(\"defense\", np.nan)\n",
    "    spa  = stats_map.get(\"special-attack\", np.nan)\n",
    "    spd  = stats_map.get(\"special-defense\", np.nan)\n",
    "    spe  = stats_map.get(\"speed\", np.nan)\n",
    "\n",
    "    types = [t[\"type\"][\"name\"] for t in poke[\"types\"]]\n",
    "    type_vec = {f\"type_{t}\": 1 if t in types else 0 for t in TYPE_ORDER}\n",
    "\n",
    "    row = {\n",
    "        \"id\": poke[\"id\"],\n",
    "        \"name\": poke[\"name\"],\n",
    "        \"hp\": hp, \"attack\": atk, \"defense\": defe, \"sp_attack\": spa, \"sp_defense\": spd, \"speed\": spe,\n",
    "        \"weight\": poke.get(\"weight\", np.nan),               # hectograms\n",
    "        \"height\": poke.get(\"height\", np.nan),               # decimeters\n",
    "        \"base_experience\": poke.get(\"base_experience\", np.nan),\n",
    "        \"generation\": species.get(\"generation\", {}).get(\"name\"),\n",
    "        \"capture_rate\": species.get(\"capture_rate\", np.nan),\n",
    "        \"egg_groups\": \",\".join(sorted([e[\"name\"] for e in species.get(\"egg_groups\", [])])),\n",
    "        \"primary_abilities\": \",\".join(primary_abilities) if primary_abilities else \"\"\n",
    "    }\n",
    "    row.update(type_vec)\n",
    "    return row\n",
    "\n",
    "def build_dataset(max_pokemon=1025, sleep_sec=0.1, cache_csv=\"pokemon_raw_cache.csv\") -> pd.DataFrame:\n",
    "    if os.path.exists(cache_csv):\n",
    "        return pd.read_csv(cache_csv)\n",
    "\n",
    "    names = fetch_all_pokemon_names(limit=max_pokemon)\n",
    "    rows = []\n",
    "    for name in tqdm(names, desc=\"Fetching Pokémon\"):\n",
    "        try:\n",
    "            poke = fetch_pokemon_detail(name)\n",
    "            species = fetch_species_detail(poke[\"species\"][\"name\"])\n",
    "            rows.append(row_from_payload(poke, species))\n",
    "            time.sleep(sleep_sec)  # polite delay\n",
    "        except Exception as e:\n",
    "            sys.stderr.write(f\"[skip] {name}: {e}\\n\")\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(cache_csv, index=False)\n",
    "    return df\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Feature engineering\n",
    "# ------------------------------\n",
    "def make_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = df.copy()\n",
    "\n",
    "    # egg_groups -> multi-hot\n",
    "    all_eggs = sorted({g for s in df[\"egg_groups\"].fillna(\"\") for g in s.split(\",\") if g})\n",
    "    for g in all_eggs:\n",
    "        df[f\"egg_{g}\"] = df[\"egg_groups\"].apply(lambda s: 1 if g in s.split(\",\") else 0)\n",
    "\n",
    "    # generation -> one-hot\n",
    "    gens = sorted(df[\"generation\"].dropna().unique().tolist())\n",
    "    for g in gens:\n",
    "        df[f\"gen_{g}\"] = (df[\"generation\"] == g).astype(int)\n",
    "\n",
    "    # 数値列・one-hot列（文字列の原カラムは入れない）\n",
    "    num_cols = [\"hp\",\"attack\",\"defense\",\"sp_attack\",\"sp_defense\",\"speed\",\n",
    "                \"weight\",\"height\",\"base_experience\",\"capture_rate\"]\n",
    "    type_cols = [c for c in df.columns if c.startswith(\"type_\")]\n",
    "    egg_cols  = [c for c in df.columns if c.startswith(\"egg_\") and c != \"egg_groups\"]\n",
    "    gen_cols  = [c for c in df.columns if c.startswith(\"gen_\")]\n",
    "\n",
    "    feature_cols = num_cols + type_cols + egg_cols + gen_cols\n",
    "\n",
    "    # 安全に数値化\n",
    "    X = (df[feature_cols]\n",
    "         .apply(pd.to_numeric, errors=\"coerce\")\n",
    "         .fillna(0.0)\n",
    "         .astype(float))\n",
    "\n",
    "    meta = df[[\"id\",\"name\",\"primary_abilities\",\"egg_groups\",\"generation\"]].copy()\n",
    "    return X, meta\n",
    "\n",
    "# ------------------------------\n",
    "# 2.5) Localization helpers (Japanese output)\n",
    "# ------------------------------\n",
    "L10N_CACHE: Dict[Tuple[str, str, str], str] = {}\n",
    "\n",
    "def _get_localized_name_from_payload(payload: dict, lang: str) -> Optional[str]:\n",
    "    for n in payload.get(\"names\", []):\n",
    "        if n.get(\"language\", {}).get(\"name\") == lang:\n",
    "            return n.get(\"name\")\n",
    "    # fallback\n",
    "    fb = \"ja-Hrkt\" if lang == \"ja\" else \"ja\"\n",
    "    for n in payload.get(\"names\", []):\n",
    "        if n.get(\"language\", {}).get(\"name\") == fb:\n",
    "            return n.get(\"name\")\n",
    "    return None\n",
    "\n",
    "def get_ability_ja(ability_en: str, lang: str = \"ja\") -> str:\n",
    "    if not ability_en:\n",
    "        return ability_en\n",
    "    key = (\"ability\", ability_en, lang)\n",
    "    if key in L10N_CACHE:\n",
    "        return L10N_CACHE[key]\n",
    "    try:\n",
    "        r = _get(f\"{BASE}/ability/{ability_en}\", timeout=20)\n",
    "        ja = _get_localized_name_from_payload(r.json(), lang) or ability_en\n",
    "    except Exception:\n",
    "        ja = ability_en\n",
    "    L10N_CACHE[key] = ja\n",
    "    return ja\n",
    "\n",
    "def get_pokemon_ja(name_en: str, lang: str = \"ja\") -> str:\n",
    "    key = (\"pokemon\", name_en, lang)\n",
    "    if key in L10N_CACHE:\n",
    "        return L10N_CACHE[key]\n",
    "    try:\n",
    "        r = _get(f\"{BASE}/pokemon-species/{name_en}\", timeout=20)\n",
    "        ja = _get_localized_name_from_payload(r.json(), lang) or name_en\n",
    "    except Exception:\n",
    "        ja = name_en\n",
    "    L10N_CACHE[key] = ja\n",
    "    return ja\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Clustering with K auto-select\n",
    "# ------------------------------\n",
    "def fit_clustering(X: pd.DataFrame, k_min=8, k_max=30, pca_dim=20, seed=42):\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "\n",
    "    # PCA（速度とノイズ低減）\n",
    "    pca = PCA(n_components=min(pca_dim, Xs.shape[1]))\n",
    "    Z = pca.fit_transform(Xs)\n",
    "\n",
    "    # K探索（silhouette最大）\n",
    "    best_k, best_score, best_model = None, -1, None\n",
    "    for k in range(k_min, min(k_max, len(X))):\n",
    "        try:\n",
    "            km = KMeans(n_clusters=k, random_state=seed, n_init=10)  # 互換性のため n_init=10\n",
    "            labels = km.fit_predict(Z)\n",
    "            if len(set(labels)) < 2:\n",
    "                continue\n",
    "            score = silhouette_score(Z, labels)\n",
    "            if score > best_score:\n",
    "                best_k, best_score, best_model = k, score, km\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if best_model is None:\n",
    "        # フォールバック（小さめK）\n",
    "        best_k = max(8, min(15, len(X)//20))\n",
    "        best_model = KMeans(n_clusters=best_k, random_state=seed, n_init=10).fit(Z)\n",
    "        labels = best_model.labels_\n",
    "        best_score = silhouette_score(Z, labels) if len(set(labels)) > 1 else -1\n",
    "    else:\n",
    "        labels = best_model.labels_\n",
    "\n",
    "    return {\n",
    "        \"scaler\": scaler,\n",
    "        \"pca\": pca,\n",
    "        \"kmeans\": best_model,\n",
    "        \"labels\": labels,\n",
    "        \"embedding\": Z,\n",
    "        \"silhouette\": float(best_score)\n",
    "    }\n",
    "\n",
    "def summarize_clusters(model_bundle: Dict, X: pd.DataFrame, meta: pd.DataFrame) -> List[Dict]:\n",
    "    labels = model_bundle[\"labels\"]\n",
    "    k = len(set(labels))\n",
    "    df = meta.copy()\n",
    "    df[\"label\"] = labels\n",
    "\n",
    "    clusters = []\n",
    "    for c in range(k):\n",
    "        sub = df[df[\"label\"] == c]\n",
    "        # 主特性（複数カンマ区切り）を展開\n",
    "        abilities = []\n",
    "        for s in sub[\"primary_abilities\"].fillna(\"\"):\n",
    "            if s:\n",
    "                abilities.extend([a.strip() for a in s.split(\",\") if a.strip()])\n",
    "        ability_counts = Counter(abilities)\n",
    "        total = sum(ability_counts.values()) if ability_counts else 0\n",
    "        top_abilities = [(a, cnt/total) for a, cnt in ability_counts.most_common(10)] if total>0 else []\n",
    "\n",
    "        ex = sub[\"name\"].tolist()\n",
    "        if len(ex) > 6:\n",
    "            random.seed(0)\n",
    "            ex = random.sample(ex, 6)\n",
    "\n",
    "        clusters.append({\n",
    "            \"cluster_id\": int(c),\n",
    "            \"size\": int(len(sub)),\n",
    "            \"top_abilities\": top_abilities,  # (ability, ratio)\n",
    "            \"examples\": ex\n",
    "        })\n",
    "    return clusters\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Save / Load\n",
    "# ------------------------------\n",
    "def save_artifacts(path=\"ability_clusters.joblib\", **kwargs):\n",
    "    joblib.dump(kwargs, path)\n",
    "    print(f\"[saved] {path}\")\n",
    "\n",
    "def load_artifacts(path=\"ability_clusters.joblib\"):\n",
    "    return joblib.load(path)\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Inference for a new Pokémon\n",
    "# ------------------------------\n",
    "def build_row_from_features(feat: dict, feature_cols: List[str]) -> pd.DataFrame:\n",
    "    # feature_cols は学習時の列順\n",
    "    row = {c: 0.0 for c in feature_cols}\n",
    "    # 数値\n",
    "    for c in [\"hp\",\"attack\",\"defense\",\"sp_attack\",\"sp_defense\",\"speed\",\"weight\",\"height\",\"base_experience\",\"capture_rate\"]:\n",
    "        if c in row:\n",
    "            row[c] = float(feat.get(c, 0) or 0)\n",
    "    # タイプ\n",
    "    for t in feat.get(\"types\", []):\n",
    "        c = f\"type_{t}\"\n",
    "        if c in row:\n",
    "            row[c] = 1.0\n",
    "    # タマゴ\n",
    "    for g in feat.get(\"egg_groups\", []):\n",
    "        c = f\"egg_{g}\"\n",
    "        if c in row:\n",
    "            row[c] = 1.0\n",
    "    # 世代\n",
    "    gen = feat.get(\"generation\")\n",
    "    if gen:\n",
    "        c = f\"gen_{gen}\"\n",
    "        if c in row:\n",
    "            row[c] = 1.0\n",
    "    return pd.DataFrame([row])\n",
    "\n",
    "def predict_cluster_and_abilities(new_feat: dict, artifacts_path=\"ability_clusters.joblib\", topk=5, out_lang: Optional[str] = None):\n",
    "    bundle = load_artifacts(artifacts_path)\n",
    "    feature_cols = bundle[\"feature_cols\"]\n",
    "    X = bundle[\"X\"]              # 学習時の特徴行列（DataFrame）\n",
    "    meta = bundle[\"meta\"]        # id, name, primary_abilities, ...\n",
    "    scaler = bundle[\"scaler\"]\n",
    "    pca = bundle[\"pca\"]\n",
    "    kmeans = bundle[\"kmeans\"]\n",
    "    clusters = bundle[\"clusters\"] # プロファイル\n",
    "\n",
    "    X_new = build_row_from_features(new_feat, feature_cols)\n",
    "    Z_new = pca.transform(scaler.transform(X_new))[0]\n",
    "    label = int(kmeans.predict([Z_new])[0])\n",
    "\n",
    "    # クラスターの特性上位\n",
    "    cluster_info = next((c for c in clusters if c[\"cluster_id\"] == label), None)\n",
    "    top_abilities = cluster_info[\"top_abilities\"][:topk] if cluster_info else []\n",
    "\n",
    "    # 近傍個体（参考提示）\n",
    "    nn = NearestNeighbors(n_neighbors=min(10, len(X)), metric=\"euclidean\")\n",
    "    Z_all = pca.transform(scaler.transform(X))\n",
    "    nn.fit(Z_all)\n",
    "    dists, idxs = nn.kneighbors([Z_new], return_distance=True)\n",
    "    neighbors = []\n",
    "    for d, i in zip(dists[0], idxs[0]):\n",
    "        neighbors.append({\n",
    "            \"name\": meta.iloc[i][\"name\"],\n",
    "            \"abilities\": meta.iloc[i][\"primary_abilities\"],\n",
    "            \"distance\": float(d)\n",
    "        })\n",
    "\n",
    "    result = {\n",
    "        \"pred_cluster\": label,\n",
    "        \"suggested_abilities\": top_abilities,  # (ability_en, ratio)\n",
    "        \"cluster_examples\": cluster_info[\"examples\"] if cluster_info else [],\n",
    "        \"nearest_neighbors\": neighbors[:10]\n",
    "    }\n",
    "\n",
    "    # --- 日本語化（任意） ---\n",
    "    if out_lang in (\"ja\", \"ja-Hrkt\"):\n",
    "        # 特性名\n",
    "        result[\"suggested_abilities\"] = [\n",
    "            (get_ability_ja(a_en, out_lang), ratio) for (a_en, ratio) in result[\"suggested_abilities\"]\n",
    "        ]\n",
    "        # 例と近傍のポケモン名\n",
    "        result[\"cluster_examples\"] = [get_pokemon_ja(n, out_lang) for n in result[\"cluster_examples\"]]\n",
    "        for n in result[\"nearest_neighbors\"]:\n",
    "            n[\"name\"] = get_pokemon_ja(n[\"name\"], out_lang)\n",
    "            if n.get(\"abilities\"):\n",
    "                en_list = [x.strip() for x in n[\"abilities\"].split(\",\") if x.strip()]\n",
    "                n[\"abilities\"] = \", \".join(get_ability_ja(x, out_lang) for x in en_list) if en_list else \"\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Main\n",
    "# ------------------------------\n",
    "def main():\n",
    "    # 学習\n",
    "    df = build_dataset()\n",
    "    X, meta = make_features(df)\n",
    "    model_bundle = fit_clustering(X, k_min=8, k_max=30, pca_dim=20, seed=42)\n",
    "    clusters = summarize_clusters(model_bundle, X, meta)\n",
    "\n",
    "    # 保存（推論に必要な最小限＋解釈用情報）\n",
    "    artifacts = {\n",
    "        \"feature_cols\": list(X.columns),\n",
    "        \"X\": X, \"meta\": meta,\n",
    "        \"scaler\": model_bundle[\"scaler\"],\n",
    "        \"pca\": model_bundle[\"pca\"],\n",
    "        \"kmeans\": model_bundle[\"kmeans\"],\n",
    "        \"clusters\": clusters,\n",
    "        \"silhouette\": model_bundle[\"silhouette\"]\n",
    "    }\n",
    "    save_artifacts(\"ability_clusters.joblib\", **artifacts)\n",
    "\n",
    "    print(f\"Silhouette: {model_bundle['silhouette']:.4f}\")\n",
    "    print(\"Cluster sizes & top abilities (head):\")\n",
    "    for c in clusters[:5]:\n",
    "        print(f\"  C{c['cluster_id']:02d} size={c['size']}, top={[(a, round(r,3)) for a,r in c['top_abilities'][:3]]}\")\n",
    "\n",
    "    # ---- デモ推論（仮の新キャラ）----\n",
    "    new_mon = {\n",
    "        \"hp\": 80, \"attack\": 105, \"defense\": 75, \"sp_attack\": 60, \"sp_defense\": 75, \"speed\": 95,\n",
    "        \"weight\": 320, \"height\": 14, \"base_experience\": 200, \"capture_rate\": 45,\n",
    "        \"types\": [\"fire\",\"fighting\"],\n",
    "        \"egg_groups\": [\"field\",\"human-like\"],\n",
    "        \"generation\": \"generation-ix\"\n",
    "    }\n",
    "    res = predict_cluster_and_abilities(new_mon, \"ability_clusters.joblib\", topk=5, out_lang=\"ja\")\n",
    "    print(\"\\n[新キャラ 推奨特性（クラスタベース）]\")\n",
    "    print(\" クラスターID:\", res[\"pred_cluster\"])\n",
    "    print(\" 推奨特性TOP5:\", [(a, round(r,3)) for a,r in res[\"suggested_abilities\"]])\n",
    "    print(\" 例ポケモン:\", res[\"cluster_examples\"])\n",
    "    print(\" 近傍（名前 / 特性 / 距離）:\")\n",
    "    for n in res[\"nearest_neighbors\"][:5]:\n",
    "        print(\"  -\", n[\"name\"], \"|\", n[\"abilities\"], \"| d=\", round(n[\"distance\"], 3))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2342ab29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Artificial_Streaming-yRA5kDWw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
